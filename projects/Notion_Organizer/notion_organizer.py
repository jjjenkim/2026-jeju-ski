#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Notion to Obsidian Organizer
Notion ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ Obsidian ë³¼íŠ¸ì— ë¦¬í¬íŠ¸ ìƒì„±
"""

import os
import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
from notion_client import Client
import time

class NotionToObsidianOrganizer:
    def __init__(self, config_path="config.txt"):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ì´ˆê¸°í™”"""
        self.config = self.load_config(config_path)
        self.notion = Client(auth=self.config['NOTION_TOKEN'])
        self.obsidian_vault = Path(self.config['OBSIDIAN_VAULT_PATH'])
        self.output_dir = self.obsidian_vault / "Notion_Index"
        self.output_dir.mkdir(exist_ok=True)
        self.snapshot_dir = Path("output")
        self.snapshot_dir.mkdir(exist_ok=True)
        self.pages_data = []
        
    def load_config(self, config_path):
        """config.txt ë¡œë“œ"""
        config = {}
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        config[key.strip()] = value.strip()
            
            # í•„ìˆ˜ ì„¤ì • í™•ì¸
            if not config.get('NOTION_TOKEN'):
                raise Exception("NOTION_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            if not config.get('OBSIDIAN_VAULT_PATH'):
                raise Exception("OBSIDIAN_VAULT_PATHê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            return config
        except Exception as e:
            raise Exception(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def search_all_pages(self):
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  í˜ì´ì§€ ê²€ìƒ‰"""
        print("\nğŸ“‚ Notion í˜ì´ì§€ ìŠ¤ìº” ì¤‘...")
        all_pages = []
        has_more = True
        start_cursor = None
        
        while has_more:
            try:
                response = self.notion.search(
                    filter={"property": "object", "value": "page"},
                    start_cursor=start_cursor,
                    page_size=100
                )
                
                all_pages.extend(response['results'])
                has_more = response['has_more']
                start_cursor = response.get('next_cursor')
                
                print(f"\rì§„í–‰: {len(all_pages)}ê°œ í˜ì´ì§€ ë°œê²¬...", end="")
                time.sleep(0.3)  # API rate limit ë°©ì§€
                
            except Exception as e:
                print(f"\nâš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        print(f"\nâœ“ {len(all_pages)}ê°œì˜ í˜ì´ì§€ ë°œê²¬\n")
        return all_pages
    
    def extract_page_metadata(self, page):
        """í˜ì´ì§€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            page_id = page['id']
            
            # ì œëª© ì¶”ì¶œ
            title = "Untitled"
            if 'properties' in page:
                title_prop = page['properties'].get('title') or page['properties'].get('Name')
                if title_prop and title_prop.get('title'):
                    title = ''.join([t['plain_text'] for t in title_prop['title']])
            
            # ìƒì„±/ìˆ˜ì • ë‚ ì§œ
            created_time = page.get('created_time', '')
            last_edited_time = page.get('last_edited_time', '')
            
            # í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë¸”ë¡ ì½ê¸°)
            content_text = self.get_page_content(page_id)
            
            # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
            word_count = len(content_text.split())
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = self.extract_tags(page, content_text)
            
            # ë§í¬ ì¶”ì¶œ
            links = self.extract_links(content_text)
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì •
            category = self.estimate_category(title, content_text, tags)
            
            # ì‹ ë¢°ë„
            confidence = self.estimate_confidence(word_count, len(links))
            
            return {
                'id': page_id,
                'title': title,
                'category': category,
                'tags': tags,
                'word_count': word_count,
                'links_out': len(links),
                'links_out_list': links[:10],
                'confidence': confidence,
                'created_time': created_time[:10] if created_time else '',
                'last_edited_time': last_edited_time[:10] if last_edited_time else '',
                'url': page.get('url', '')
            }
        except Exception as e:
            return None
    
    def get_page_content(self, page_id):
        """í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            blocks = self.notion.blocks.children.list(page_id)
            content = []
            
            for block in blocks['results']:
                block_type = block['type']
                if block_type in ['paragraph', 'heading_1', 'heading_2', 'heading_3', 
                                 'bulleted_list_item', 'numbered_list_item', 'to_do', 'quote']:
                    text_content = block.get(block_type, {}).get('rich_text', [])
                    for text in text_content:
                        content.append(text.get('plain_text', ''))
            
            return ' '.join(content)
        except:
            return ""
    
    def extract_tags(self, page, content):
        """íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        
        # ì†ì„±ì—ì„œ íƒœê·¸ ì¶”ì¶œ
        if 'properties' in page:
            for prop_name, prop_value in page['properties'].items():
                if prop_value['type'] == 'multi_select':
                    tags.extend([tag['name'] for tag in prop_value.get('multi_select', [])])
        
        # ë‚´ìš©ì—ì„œ #íƒœê·¸ ì¶”ì¶œ
        hash_tags = re.findall(r'#(\w+)', content)
        tags.extend(hash_tags)
        
        return list(set(tags))[:5]
    
    def extract_links(self, content):
        """Notion í˜ì´ì§€ ë§í¬ ì¶”ì¶œ"""
        links = re.findall(r'https://www\.notion\.so/[\w-]+', content)
        return list(set(links))
    
    def estimate_category(self, title, content, tags):
        """ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì •"""
        title_lower = title.lower()
        content_lower = content[:1000].lower()
        tags_str = ' '.join(tags).lower()
        
        # ğŸ¤– AI & Tools
        ai_keywords = ['gpt', 'chatgpt', 'claude', 'perplexity', 'ai', 'í”„ë¡¬í”„íŠ¸', 'prompt', 
                       'gemini', 'anthropic', 'llm', 'notebooklm', 'ìë™í™”', 'automation']
        if any(word in title_lower or word in content_lower or word in tags_str 
               for word in ai_keywords):
            return 'AI & Tools'
        
        # ğŸ—£ï¸ í†µì—­ & ì–¸ì–´
        interp_keywords = ['í†µì—­', 'ìˆœì°¨í†µì—­', 'ë™ì‹œí†µì—­', 'interpretation', 'interpreter', 
                           'í‘œí˜„', 'ì˜ì–´', 'english', 'í†µëŒ€', 'ì–¸ì–´', 'ë°œìŒ', 'ë¬¸ë²•', 
                           'broca', 'note-taking', 'ë©”ëª¨ë¦¬', 'êµ¬ìˆ ', 'ë²ˆì—­']
        if any(word in title_lower or word in content_lower or word in tags_str 
               for word in interp_keywords):
            return 'Language & Interpretation'
        
        # ğŸ”ï¸ ìŠ¤í¬ì¸ 
        sports_keywords = ['fis', 'mogul', 'freestyle', 'ìŠ¤í‚¤', 'ìŠ¤ë…¸ë³´ë“œ', 'halfpipe', 
                           'ì˜¬ë¦¼í”½', 'olympic', 'world cup', 'upshot', 'ì„ ìˆ˜', 'athlete',
                           'ì›”ë“œì»µ', 'í•˜í”„íŒŒì´í”„', 'ëª¨ê¸€']
        if any(word in title_lower or word in content_lower or word in tags_str 
               for word in sports_keywords):
            return 'Winter Sports'
        
        # ğŸ“° ë¯¸ë””ì–´ & ë‰´ìŠ¤ë ˆí„°
        media_keywords = ['ë‰´ìŠ¤ë ˆí„°', 'newsletter', 'ë©”ì¼ë¦¬', 'upshot', 'template', 
                          'seo', 'ì½˜í…ì¸ ', 'content', 'ë°œí–‰', 'êµ¬ë…', 'cover story']
        if any(word in title_lower or word in content_lower or word in tags_str 
               for word in media_keywords):
            return 'Media & Newsletter'
        
        # ğŸ“ í•™ì—… & ì—°êµ¬
        academic_keywords = ['ë…¼ë¬¸', 'thesis', 'ì—°êµ¬', 'research', 'í•™ìŠµ', 'ê³µë¶€', 
                            'ìŠ¤í„°ë””', 'study', 'ì™¸ëŒ€', 'í†µë²ˆì—­', 'ìˆ˜ì—…', 'diplomacy']
        if any(word in title_lower or word in content_lower or word in tags_str 
               for word in academic_keywords):
            return 'Academic & Research'
        
        # ğŸ“ Notes
        if any(word in title_lower 
               for word in ['today', 'ë©”ëª¨', 'note', 'ì„ì‹œ', 'temp', 'ì´ˆì•ˆ', 'draft', 'ìƒê°']):
            return 'Notes'
        
        # ğŸš€ Projects
        if any(word in title_lower or word in content_lower 
               for word in ['í”„ë¡œì íŠ¸', 'project', 'ì§„í–‰', 'todo', 'task', 'mvp', 'êµ¬í˜„']):
            return 'Projects'
        
        # ğŸ”— Resources
        if any(word in title_lower or word in content_lower 
               for word in ['ë§í¬', 'link', 'ìë£Œ', 'resource', 'reference', 'ê°€ì´ë“œ', 'guide']):
            return 'Resources'
        
        return 'General Knowledge'
    
    def estimate_confidence(self, word_count, link_count):
        """ë¬¸ì„œ ì™„ì„±ë„ ì¶”ì •"""
        if word_count < 50:
            return 'draft'
        elif word_count < 300:
            return 'partial'
        else:
            return 'complete'
    
    def calculate_links_in(self):
        """ì—­ë§í¬ ê³„ì‚°"""
        links_in_map = defaultdict(int)
        
        for page_data in self.pages_data:
            for link in page_data['links_out_list']:
                links_in_map[link] += 1
        
        for page_data in self.pages_data:
            page_id = page_data['id']
            page_data['links_in'] = links_in_map.get(page_id, 0)
    
    def load_from_snapshot(self):
        """ìµœì‹  ìŠ¤ëƒ…ìƒ·ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        snapshots = list(self.snapshot_dir.glob("snapshot_*.json"))
        if not snapshots:
            return False
        
        latest = max(snapshots, key=lambda x: x.stat().st_mtime)
        print(f"\nğŸ“¦ ìŠ¤ëƒ…ìƒ· ë¡œë“œ ì¤‘: {latest.name}")
        
        try:
            with open(latest, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.pages_data = data['pages']
                print(f"âœ“ {len(self.pages_data)}ê°œ í˜ì´ì§€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n")
                return True
        except Exception as e:
            print(f"âš ï¸ ìŠ¤ëƒ…ìƒ· ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def incremental_update(self):
        """ì¦ë¶„ ì—…ë°ì´íŠ¸: ê¸°ì¡´ ë°ì´í„° + ìƒˆë¡œìš´/ìˆ˜ì •ëœ í˜ì´ì§€ë§Œ ìŠ¤ìº”"""
        # ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· ë¡œë“œ
        if not self.load_from_snapshot():
            print("âš ï¸ ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ìŠ¤ìº”ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤...\n")
            self.analyze_workspace(use_snapshot=False)
            return
        
        old_pages_data = self.pages_data.copy()
        old_pages_map = {p['id']: p for p in old_pages_data}
        
        print("ğŸ”„ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        print(f"ê¸°ì¡´ ë°ì´í„°: {len(old_pages_data)}ê°œ í˜ì´ì§€\n")
        
        # ì „ì²´ í˜ì´ì§€ IDë§Œ ë¹ ë¥´ê²Œ ìŠ¤ìº”
        print("ğŸ“‚ Notion í˜ì´ì§€ ID ìŠ¤ìº” ì¤‘...")
        all_pages = self.search_all_pages()
        current_page_ids = {page['id'] for page in all_pages}
        
        # ìƒˆë¡œìš´ í˜ì´ì§€, ìˆ˜ì •ëœ í˜ì´ì§€, ì‚­ì œëœ í˜ì´ì§€ ì°¾ê¸°
        new_pages = []
        updated_pages = []
        deleted_page_ids = []
        
        for page in all_pages:
            page_id = page['id']
            last_edited = page.get('last_edited_time', '')
            
            if page_id not in old_pages_map:
                new_pages.append(page)
            elif old_pages_map[page_id]['last_edited_time'] != last_edited[:10]:
                updated_pages.append(page)
        
        # ì‚­ì œëœ í˜ì´ì§€ ì°¾ê¸°
        for old_page_id in old_pages_map.keys():
            if old_page_id not in current_page_ids:
                deleted_page_ids.append(old_page_id)
        
        print(f"\nğŸ“Š ë°œê²¬ëœ ë³€ê²½ì‚¬í•­:")
        print(f"  - ìƒˆ í˜ì´ì§€: {len(new_pages)}ê°œ")
        print(f"  - ìˆ˜ì •ëœ í˜ì´ì§€: {len(updated_pages)}ê°œ")
        print(f"  - ì‚­ì œëœ í˜ì´ì§€: {len(deleted_page_ids)}ê°œ")
        print(f"  - ë³€ê²½ ì—†ìŒ: {len(all_pages) - len(new_pages) - len(updated_pages)}ê°œ\n")
        
        # ì‚­ì œëœ í˜ì´ì§€ ì œê±°
        if deleted_page_ids:
            print(f"ğŸ—‘ï¸ ì‚­ì œëœ í˜ì´ì§€ ì œê±° ì¤‘...")
            for deleted_id in deleted_page_ids:
                del old_pages_map[deleted_id]
            print(f"âœ“ {len(deleted_page_ids)}ê°œ í˜ì´ì§€ ì œê±° ì™„ë£Œ\n")
        
        # ìƒˆë¡œìš´/ìˆ˜ì •ëœ í˜ì´ì§€ë§Œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        to_process = new_pages + updated_pages
        if to_process:
            print("ğŸ“Š ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            new_data = []
            for i, page in enumerate(to_process, 1):
                print(f"\rì§„í–‰: {i}/{len(to_process)} ({i/len(to_process)*100:.1f}%)", end="")
                metadata = self.extract_page_metadata(page)
                if metadata and metadata['word_count'] >= 10:
                    new_data.append(metadata)
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                    old_pages_map[metadata['id']] = metadata
                time.sleep(0.1)
            
            print(f"\nâœ“ {len(new_data)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ\n")
        
        # ë³‘í•©ëœ ë°ì´í„°
        self.pages_data = list(old_pages_map.values())
        
        # ì—­ë§í¬ ì¬ê³„ì‚°
        self.calculate_links_in()
        
        # ìƒˆ ìŠ¤ëƒ…ìƒ· ì €ì¥
        self.save_snapshot()
        
        print(f"âœ“ ìµœì¢… ë°ì´í„°: {len(self.pages_data)}ê°œ í˜ì´ì§€")
        if deleted_page_ids or new_pages or updated_pages:
            print(f"   ({len(old_pages_data)}ê°œ â†’ {len(self.pages_data)}ê°œ)")
    
    def analyze_workspace(self, use_snapshot=True):
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì „ì²´ ë¶„ì„"""
        # ìŠ¤ëƒ…ìƒ· ì‚¬ìš© ì‹œë„
        if use_snapshot and self.load_from_snapshot():
            print("âœ“ ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· ì‚¬ìš© (Notion API í˜¸ì¶œ ì—†ìŒ)\n")
            return
        
        # ìŠ¤ëƒ…ìƒ·ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìŠ¤ìº”
        print("âš ï¸ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤. Notion APIë¡œ ìƒˆë¡œ ìŠ¤ìº”í•©ë‹ˆë‹¤...\n")
        pages = self.search_all_pages()
        
        print("ğŸ“Š ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        for i, page in enumerate(pages, 1):
            print(f"\rì§„í–‰: {i}/{len(pages)} ({i/len(pages)*100:.1f}%)", end="")
            metadata = self.extract_page_metadata(page)
            if metadata and metadata['word_count'] >= 10:
                self.pages_data.append(metadata)
            time.sleep(0.1)
        
        print(f"\nâœ“ {len(self.pages_data)}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ\n")
        
        # ì—­ë§í¬ ê³„ì‚°
        self.calculate_links_in()
        
        # ìŠ¤ëƒ…ìƒ· ì €ì¥
        self.save_snapshot()
    
    def save_snapshot(self):
        """ë¶„ì„ ê²°ê³¼ ìŠ¤ëƒ…ìƒ· ì €ì¥"""
        snapshot_file = self.snapshot_dir / f"snapshot_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        
        with open(snapshot_file, 'w', encoding='utf-8') as f:
            json.dump({
                'date': datetime.now().isoformat(),
                'total_pages': len(self.pages_data),
                'pages': self.pages_data
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ìŠ¤ëƒ…ìƒ· ì €ì¥: {snapshot_file.name}")
    
    def save_markdown(self, filename, content):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥"""
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ“ {filename} ìƒì„±")
    
    def generate_all_reports(self):
        """ëª¨ë“  ë¦¬í¬íŠ¸ ìƒì„± (Obsidian ë§ˆí¬ë‹¤ìš´)"""
        print(f"\nğŸ“ Obsidian ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        print(f"ì¶œë ¥ ê²½ë¡œ: {self.output_dir}\n")
        
        self.create_master_index()
        self.create_categories_index()
        self.create_tags_index()
        self.create_action_queue()
        self.create_statistics()
        self.create_smart_filters()
        self.create_frequent_phrases()
        self.create_moc_hub()
        
        print("\nâœ“ ëª¨ë“  ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    
    def create_master_index(self):
        """00_Master_Index.md ìƒì„±"""
        total = len(self.pages_data)
        by_category = Counter(p['category'] for p in self.pages_data)
        
        content = []
        content.append("# ğŸ“Š Notion Workspace Master Index\n")
        content.append(f"> ìµœì¢… ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        
        content.append("## ğŸ“ˆ í†µê³„ ìš”ì•½\n")
        content.append(f"- ì´ ë¬¸ì„œ: {total}ê°œ\n")
        
        emoji_map = {
            'AI & Tools': 'ğŸ¤–',
            'Language & Interpretation': 'ğŸ—£ï¸',
            'Winter Sports': 'ğŸ”ï¸',
            'Media & Newsletter': 'ğŸ“°',
            'Academic & Research': 'ğŸ“',
            'Projects': 'ğŸš€',
            'Notes': 'ğŸ“',
            'Resources': 'ğŸ”—',
            'General Knowledge': 'ğŸ“š'
        }
        
        for cat, count in by_category.most_common():
            emoji = emoji_map.get(cat, 'ğŸ“„')
            content.append(f"- {emoji} {cat}: {count}ê°œ\n")
        
        content.append("\n## ğŸ“Œ ìµœê·¼ ìˆ˜ì • (TOP 10)\n")
        recent = sorted(self.pages_data, key=lambda x: x['last_edited_time'], reverse=True)[:10]
        for p in recent:
            content.append(f"- [{p['title']}]({p['url']}) - {p['last_edited_time']} ({p['word_count']} words)\n")
        
        content.append("\n## ğŸ”¥ í—ˆë¸Œ ë¬¸ì„œ (ë§í¬ ë§ìŒ)\n")
        hubs = sorted(self.pages_data, key=lambda x: x['links_in'], reverse=True)[:10]
        for p in hubs:
            if p['links_in'] > 0:
                content.append(f"- [{p['title']}]({p['url']}) ({p['links_in']}ê°œ ë§í¬)\n")
        
        self.save_markdown("00_Master_Index.md", ''.join(content))
    
    def create_categories_index(self):
        """01_Categories.md ìƒì„±"""
        content = []
        content.append("# ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ëª©ë¡\n")
        
        by_category = defaultdict(list)
        for p in self.pages_data:
            by_category[p['category']].append(p)
        
        categories_order = [
            ('AI & Tools', 'ğŸ¤–'),
            ('Language & Interpretation', 'ğŸ—£ï¸'),
            ('Winter Sports', 'ğŸ”ï¸'),
            ('Media & Newsletter', 'ğŸ“°'),
            ('Academic & Research', 'ğŸ“'),
            ('Projects', 'ğŸš€'),
            ('Notes', 'ğŸ“'),
            ('Resources', 'ğŸ”—'),
            ('General Knowledge', 'ğŸ“š')
        ]
        
        for cat, emoji in categories_order:
            pages = by_category.get(cat, [])
            if not pages:
                continue
            
            content.append(f"\n## {emoji} {cat} ({len(pages)}ê°œ)\n")
            
            for p in sorted(pages, key=lambda x: x['word_count'], reverse=True)[:30]:
                tags_str = ' '.join(f'#{t}' for t in p['tags'][:3])
                content.append(f"- [{p['title']}]({p['url']}) - {p['word_count']} words {tags_str}\n")
        
        self.save_markdown("01_Categories.md", ''.join(content))
    
    def create_tags_index(self):
        """02_Tags_System.md ìƒì„±"""
        content = []
        content.append("# ğŸ·ï¸ íƒœê·¸ ì‹œìŠ¤í…œ\n")
        
        tag_pages = defaultdict(list)
        for p in self.pages_data:
            for tag in p['tags']:
                tag_pages[tag].append(p)
        
        if not tag_pages:
            content.append("\níƒœê·¸ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
        else:
            content.append("\n## ğŸ“Š íƒœê·¸ ì‚¬ìš© ë¹ˆë„ TOP 30\n")
            
            for tag, pages in sorted(tag_pages.items(), key=lambda x: len(x[1]), reverse=True)[:30]:
                content.append(f"\n### #{tag} ({len(pages)}ê°œ)\n")
                for p in pages[:10]:
                    content.append(f"- [{p['title']}]({p['url']})\n")
                if len(pages) > 10:
                    content.append(f"- ... ì™¸ {len(pages)-10}ê°œ\n")
        
        self.save_markdown("02_Tags_System.md", ''.join(content))
    
    def create_action_queue(self):
        """03_Action_Queue.md ìƒì„±"""
        content = []
        content.append("# ğŸ”§ ì •ë¦¬ ì•¡ì…˜ ì œì•ˆ\n")
        content.append(f"> ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        
        content.append("\n## ğŸ—‘ï¸ Delete í›„ë³´ (ì§§ê³  ë§í¬ ì—†ìŒ)\n")
        delete_candidates = [p for p in self.pages_data 
                            if p['word_count'] < 50 and p['links_in'] == 0 and p['links_out'] == 0]
        
        if delete_candidates:
            for p in delete_candidates[:20]:
                content.append(f"- [ ] [{p['title']}]({p['url']}) ({p['word_count']} words)\n")
        else:
            content.append("ì—†ìŒ\n")
        
        content.append("\n## âœ‚ï¸ Split í›„ë³´ (3000+ words)\n")
        split_candidates = [p for p in self.pages_data if p['word_count'] > 3000]
        if split_candidates:
            for p in split_candidates[:20]:
                content.append(f"- [ ] [{p['title']}]({p['url']}) ({p['word_count']} words)\n")
        else:
            content.append("ì—†ìŒ\n")
        
        self.save_markdown("03_Action_Queue.md", ''.join(content))
    
    def create_statistics(self):
        """04_Statistics.md ìƒì„±"""
        content = []
        content.append("# ğŸ“Š í†µê³„ ë° ë¶„ì„\n")
        
        by_category = Counter(p['category'] for p in self.pages_data)
        content.append("\n## ì¹´í…Œê³ ë¦¬ ë¶„í¬\n")
        for cat, count in by_category.most_common():
            pct = count / len(self.pages_data) * 100
            content.append(f"- {cat}: {count}ê°œ ({pct:.1f}%)\n")
        
        # íƒœê·¸ TOP 10
        all_tags = []
        for p in self.pages_data:
            all_tags.extend(p['tags'])
        
        if all_tags:
            tag_counts = Counter(all_tags)
            content.append("\n## ìì£¼ ì“°ëŠ” íƒœê·¸ TOP 20\n")
            for tag, count in tag_counts.most_common(20):
                content.append(f"- #{tag}: {count}íšŒ\n")
        
        # ë¬¸ì„œ í¬ê¸° ë¶„í¬
        content.append("\n## ë¬¸ì„œ í¬ê¸° ë¶„í¬\n")
        word_counts = [p['word_count'] for p in self.pages_data]
        if word_counts:
            content.append(f"- í‰ê· : {sum(word_counts)/len(word_counts):.0f} words\n")
            content.append(f"- ìµœëŒ€: {max(word_counts)} words\n")
            content.append(f"- ìµœì†Œ: {min(word_counts)} words\n")
        
        self.save_markdown("04_Statistics.md", ''.join(content))
    
    def create_smart_filters(self):
        """05_Smart_Filters.md ìƒì„±"""
        content = []
        content.append("# âš¡ Smart Filters - ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ í•„í„°\n")
        
        content.append("\n## ğŸ”¥ Long Form (2000+ words)\n")
        long_docs = [p for p in self.pages_data if p['word_count'] >= 2000]
        for p in sorted(long_docs, key=lambda x: x['word_count'], reverse=True)[:20]:
            content.append(f"- [{p['title']}]({p['url']}) - {p['word_count']} words\n")
        
        content.append("\n## ğŸ“ Medium (500-2000 words)\n")
        medium_docs = [p for p in self.pages_data if 500 <= p['word_count'] < 2000]
        for p in sorted(medium_docs, key=lambda x: x['word_count'], reverse=True)[:20]:
            content.append(f"- [{p['title']}]({p['url']}) - {p['word_count']} words\n")
        
        content.append("\n## ğŸ’¡ Short Notes (100-500 words)\n")
        short_docs = [p for p in self.pages_data if 100 <= p['word_count'] < 500]
        for p in sorted(short_docs, key=lambda x: x['word_count'], reverse=True)[:20]:
            content.append(f"- [{p['title']}]({p['url']}) - {p['word_count']} words\n")
        
        self.save_markdown("05_Smart_Filters.md", ''.join(content))
    
    def create_frequent_phrases(self):
        """06_Frequent_Phrases.md ìƒì„±"""
        content = []
        content.append("# ğŸ’¬ ìì£¼ ì“°ëŠ” í‘œí˜„ & í‚¤ì›Œë“œ ë¶„ì„\n")
        
        all_words = []
        for p in self.pages_data:
            words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', p['title'])
            all_words.extend(words)
        
        stopwords = ['the', 'and', 'for', 'with', 'this', 'that']
        all_words = [w for w in all_words if w.lower() not in stopwords]
        
        word_counts = Counter(all_words)
        
        content.append("\n## ğŸ“Š ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ TOP 30\n")
        for word, count in word_counts.most_common(30):
            content.append(f"- **{word}**: {count}íšŒ\n")
        
        self.save_markdown("06_Frequent_Phrases.md", ''.join(content))
    
    def create_moc_hub(self):
        """07_MOC_Hub.md ìƒì„±"""
        content = []
        content.append("# ğŸ—ºï¸ Map of Contents - ì£¼ì œë³„ í—ˆë¸Œ\n")
        
        content.append("\n## ğŸ”ï¸ ê²¨ìš¸ ìŠ¤í¬ì¸  í—ˆë¸Œ\n")
        sports_files = [p for p in self.pages_data if p['category'] == 'Winter Sports']
        for p in sorted(sports_files, key=lambda x: x['word_count'], reverse=True)[:15]:
            content.append(f"- [{p['title']}]({p['url']}) ({p['word_count']} words)\n")
        
        content.append("\n## ğŸ—£ï¸ í†µì—­ & ì–¸ì–´ í•™ìŠµ í—ˆë¸Œ\n")
        lang_files = [p for p in self.pages_data if p['category'] == 'Language & Interpretation']
        for p in sorted(lang_files, key=lambda x: x['word_count'], reverse=True)[:15]:
            content.append(f"- [{p['title']}]({p['url']}) ({p['word_count']} words)\n")
        
        content.append("\n## ğŸ¤– AI & Tools í—ˆë¸Œ\n")
        ai_files = [p for p in self.pages_data if p['category'] == 'AI & Tools']
        for p in sorted(ai_files, key=lambda x: x['word_count'], reverse=True)[:15]:
            content.append(f"- [{p['title']}]({p['url']}) ({p['word_count']} words)\n")
        
        self.save_markdown("07_MOC_Hub.md", ''.join(content))


def main():
    print("\n" + "="*60)
    print("ğŸš€ Notion to Obsidian Organizer")
    print("="*60 + "\n")
    
    try:
        # ì´ˆê¸°í™”
        organizer = NotionToObsidianOrganizer()
        
        # ìŠ¤ëƒ…ìƒ· í™•ì¸
        snapshots = list(organizer.snapshot_dir.glob("snapshot_*.json"))
        
        if snapshots:
            latest = max(snapshots, key=lambda x: x.stat().st_mtime)
            snapshot_date = datetime.fromtimestamp(latest.stat().st_mtime)
            
            print(f"ğŸ“¦ ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· ë°œê²¬: {latest.name}")
            print(f"   ìƒì„±ì¼: {snapshot_date.strftime('%Y-%m-%d %H:%M')}")
            print(f"   í¬ê¸°: {latest.stat().st_size / 1024:.1f} KB\n")
            
            print("ì–´ë–»ê²Œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\n")
            print("1ï¸âƒ£  ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· ì‚¬ìš©")
            print("   â†’ ì €ì¥ëœ ë°ì´í„°ë¡œ ë¦¬í¬íŠ¸ë§Œ ì¬ìƒì„± (Notion API í˜¸ì¶œ ì—†ìŒ)")
            print("   â†’ ì†Œìš” ì‹œê°„: 0ì´ˆ")
            print("   â†’ ì–¸ì œ: ë¦¬í¬íŠ¸ë§Œ ë‹¤ì‹œ ë³´ê³  ì‹¶ì„ ë•Œ\n")
            
            print("2ï¸âƒ£  ì¦ë¶„ ì—…ë°ì´íŠ¸ (ì¶”ì²œ)")
            print("   â†’ ê¸°ì¡´ ë°ì´í„° ìœ ì§€ + ìƒˆ/ìˆ˜ì • í˜ì´ì§€ë§Œ ìŠ¤ìº”")
            print("   â†’ ì†Œìš” ì‹œê°„: 5-10ë¶„")
            print("   â†’ ì–¸ì œ: ë¬¸ì„œë¥¼ ì¶”ê°€/ìˆ˜ì •í–ˆì„ ë•Œ\n")
            
            print("3ï¸âƒ£  ì „ì²´ ìƒˆë¡œ ìŠ¤ìº”")
            print("   â†’ ëª¨ë“  í˜ì´ì§€ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìŠ¤ìº”")
            print("   â†’ ì†Œìš” ì‹œê°„: 60-90ë¶„")
            print("   â†’ ì–¸ì œ: ì™„ì „íˆ ìƒˆë¡œ ì‹œì‘í•˜ê³  ì‹¶ì„ ë•Œ\n")
            
            choice = input("ì„ íƒ (1, 2, ë˜ëŠ” 3): ").strip()
            
            start_time = datetime.now()
            
            if choice == "2":
                print("\nâœ“ ì¦ë¶„ ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
                organizer.incremental_update()
            elif choice == "3":
                print("\nâœ“ Notion APIë¡œ ì „ì²´ ìƒˆë¡œ ìŠ¤ìº”í•©ë‹ˆë‹¤...\n")
                organizer.analyze_workspace(use_snapshot=False)
            else:
                print("\nâœ“ ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...\n")
                organizer.analyze_workspace(use_snapshot=True)
        else:
            print("âš ï¸ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤. Notion APIë¡œ ìƒˆë¡œ ìŠ¤ìº”í•©ë‹ˆë‹¤...\n")
            start_time = datetime.now()
            organizer.analyze_workspace(use_snapshot=False)
        
        # ë¦¬í¬íŠ¸ ìƒì„± (Obsidian ë§ˆí¬ë‹¤ìš´)
        organizer.generate_all_reports()
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        print("\n" + "="*60)
        print("âœ… ì™„ë£Œ!")
        print("="*60)
        print(f"ì²˜ë¦¬ëœ í˜ì´ì§€: {len(organizer.pages_data)}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"ì¶œë ¥ ê²½ë¡œ: {organizer.output_dir}")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    input("\nì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ ì¢…ë£Œ...")


if __name__ == "__main__":
    main()
