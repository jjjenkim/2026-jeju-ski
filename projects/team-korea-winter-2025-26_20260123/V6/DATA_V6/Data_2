import requests
from bs4 import BeautifulSoup
import json
import time
import random
import os
import re

# =========================================================
# âš™ï¸ ì„¤ì •
# =========================================================
INPUT_FILE = "athlete_urls.txt"
OUTPUT_JSON = "team_korea_data.json"

# âœ… 1. ì¢…ëª© ì‚¬ì „ (ì´ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì¢…ëª©ìœ¼ë¡œ ì¸ì‹)
VALID_DISCIPLINES = [
    "Moguls", "Dual Moguls", "Dual Moguls Team", "Aerials", "Aerials Team",
    "Ski Cross", "Ski Cross Team", "Freeski Halfpipe", "Freeski Slopestyle",
    "Freeski Big Air", "Snowboard Cross", "Snowboard Cross Team",
    "Giant Slalom", "Slalom", "Super G", "Downhill", "Alpine Combined",
    "Parallel Giant Slalom", "Parallel Slalom", "Parallel Giant Slalom Team"
]

# âœ… 2. ë“±ê¸‰ ì‚¬ì „ (ì´ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ë“±ê¸‰ìœ¼ë¡œ ì¸ì‹)
VALID_CATEGORIES = [
    "WC", "WSC", "FIS", "NC", "EC", "YOG", "WJC", "AC", "OPN", "NAC", "SAC", "ANC", "FEC"
]

def parse_row_text(cols):
    """5ë‹¨ê³„ í•„í„°ë§ ë¡œì§ êµ¬í˜„ë¶€"""
    data = {
        "date": "-", "place": "-", "discipline": "Unknown", 
        "category": "-", "rank": 0
    }
    
    # í•´ë‹¹ ì¤„ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    all_texts = [col.get_text(strip=True) for col in cols]
    if not all_texts: return None

    # [Rule 1] ë‚ ì§œ: ì²« ë²ˆì§¸ ì¹¸ ê³ ì •
    data['date'] = all_texts[0]

    # í…ìŠ¤íŠ¸ ì „ìˆ˜ ì¡°ì‚¬
    for text in all_texts:
        clean_text = text.strip()
        
        # [Rule 2] ì¢…ëª© ì°¾ê¸°
        for disc in VALID_DISCIPLINES:
            if disc.lower() == clean_text.lower():
                data['discipline'] = disc
                break
        
        # [Rule 3] ë“±ê¸‰ ì°¾ê¸°
        if clean_text in VALID_CATEGORIES:
            data['category'] = clean_text

    # [Rule 4] ì¥ì†Œ ì°¾ê¸° (ì†Œê±°ë²•: ë‚ ì§œ, ì¢…ëª©, ë“±ê¸‰, ìˆœìœ„ ì•„ë‹Œ ê²ƒ ì¤‘ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸)
    candidates = []
    for text in all_texts:
        t = text.strip()
        if (t != data['date'] and 
            t != data['discipline'] and 
            t != data['category'] and 
            not t.isdigit() and 
            len(t) > 2): # ë„ˆë¬´ ì§§ì€ ì¡ìŒ ì œì™¸
            candidates.append(t)
    
    if candidates:
        data['place'] = candidates[0] # ë³´í†µ ì•ì— ë‚˜ì˜¤ëŠ”ê²Œ ì¥ì†Œ

    # [Rule 5] ìˆœìœ„: ë§ˆì§€ë§‰ ì¹¸ì˜ ì²« ë²ˆì§¸ ìˆ«ì
    try:
        score_box = cols[-1].find_all("div", recursive=False)
        rank_text = score_box[0].get_text(strip=True) if score_box else "0"
        
        if rank_text.isdigit():
            data['rank'] = int(rank_text)
        else:
            return None # ìˆœìœ„ ì—†ìœ¼ë©´(DNF, DNS) ë°ì´í„°ì—ì„œ ì œì™¸
    except:
        return None

    return data

def get_photo_url(soup):
    """ì‚¬ì§„ URL ì¶”ì¶œ"""
    img_div = soup.find("div", class_="avatar__image")
    if img_div and 'style' in img_div.attrs:
        match = re.search(r"url\('([^']+)'\)", img_div['style'])
        if match: return match.group(1)
    return "https://via.placeholder.com/150?text=No+Image"

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"ğŸš¨ íŒŒì¼ ì—†ìŒ: {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"ğŸ”¥ ë°ì´í„° ì •ë°€ ì¶”ì¶œ ì‹œì‘ (ëŒ€ìƒ: {len(urls)}ëª…)")
    extracted_data = []
    
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    for idx, url in enumerate(urls):
        print(f"[{idx+1}/{len(urls)}] ì²˜ë¦¬ ì¤‘... {url}")
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200: continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í”„ë¡œí•„
            name = soup.find("h1", class_="athlete-profile__name").get_text(" ", strip=True)
            country_tag = soup.select_one(".athlete-profile__country .country__name")
            country = country_tag.get_text(strip=True) if country_tag else "-"
            photo = get_photo_url(soup)

            # ê¸°ë¡ ì¶”ì¶œ (í•„í„°ë§ ì ìš©)
            records = []
            rows = soup.find_all("a", class_="table-row")
            for row in rows:
                container = row.find("div", class_="container")
                if not container: continue
                
                cols = container.find_all("div", recursive=False)
                row_data = parse_row_text(cols) # ìœ„ì—ì„œ ì •ì˜í•œ 5ë‹¨ê³„ í•„í„° í•¨ìˆ˜ ì‚¬ìš©
                
                if row_data:
                    records.append(row_data)
            
            records.reverse() # ê³¼ê±° -> í˜„ì¬ ìˆœ ì •ë ¬

            extracted_data.append({
                "id": idx,
                "name": name,
                "country": country,
                "photo": photo,
                "records": records
            })
            
            time.sleep(random.uniform(0.1, 0.3))

        except Exception as e:
            print(f"âŒ ì—ëŸ¬: {e}")

    # JSON ì €ì¥
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(extracted_data, f, ensure_ascii=False, indent=4)

    print(f"\nâœ… ì™„ë£Œ. '{OUTPUT_JSON}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
    