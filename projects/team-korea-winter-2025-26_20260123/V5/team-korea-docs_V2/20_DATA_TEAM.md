# 20_DATA_TEAM.md
**Data Team Agents**  
**ë°ì´í„° ìˆ˜ì§‘, ì²˜ë¦¬, ê²€ì¦**

---

## ğŸ¯ íŒ€ ë¯¸ì…˜

FIS ê³µì‹ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ì œí•˜ì—¬ **ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”** ì„ ìˆ˜ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ‘¥ íŒ€ êµ¬ì„±

### Agent A: FIS ìŠ¤í¬ë˜í¼
**ì—­í• **: ì„ ìˆ˜ë³„ FIS í˜ì´ì§€ í¬ë¡¤ë§ ë° ë°ì´í„° ì¶”ì¶œ

### Agent B: ë°ì´í„° ì •ì œ
**ì—­í• **: ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦, í‘œì¤€í™”, í†µê³„ ê³„ì‚°

---

## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì „ëµ

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

```
1ì°¨: FIS API ì‹œë„
    â†“ (ì‹¤íŒ¨ ì‹œ)
2ì°¨: ì›¹ ìŠ¤í¬ë˜í•‘
    â†“
ê²€ì¦ ë° ë³‘í•©
    â†“
ìµœì¢… JSON ìƒì„±
```

---

## ğŸ” Agent A: FIS ìŠ¤í¬ë˜í¼

### ì…ë ¥ ë°ì´í„°

**ì„ ìˆ˜ URL ë¦¬ìŠ¤íŠ¸** (ì˜ˆì‹œ):
```
https://www.fis-ski.com/DB/general/athlete.html?sectorcode=FS&competitorid=123456
https://www.fis-ski.com/DB/general/athlete.html?sectorcode=SB&competitorid=789012
...
```

### í¬ë¡¤ë§ ëŒ€ìƒ ì •ë³´

**ì„ ìˆ˜ í”„ë¡œí•„**:
- ì´ë¦„ (ì˜ë¬¸, í•œê¸€ ê°€ëŠ¥í•˜ë©´)
- ìƒë…„ì›”ì¼
- ì„±ë³„
- êµ­ì 
- FIS ì½”ë“œ

**ì„±ì  ë°ì´í„°**:
- í˜„ì¬ ì‹œì¦Œ ìˆœìœ„
- ì—­ëŒ€ ìµœê³  ìˆœìœ„
- ì¶œì „ ê²½ê¸° ìˆ˜
- ë©”ë‹¬ íšë“ (ê¸ˆ/ì€/ë™)

**ìµœê·¼ ê²°ê³¼** (ìµœëŒ€ 5ê²½ê¸°):
- ëŒ€íšŒ ë‚ ì§œ
- ëŒ€íšŒëª…
- ìˆœìœ„
- í¬ì¸íŠ¸

### êµ¬í˜„ ì˜ˆì‹œ (Python)

```python
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time

class FISScraper:
    """FIS ì„ ìˆ˜ ë°ì´í„° ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, cache_file="data/cache/scraper_cache.json"):
        self.cache_file = cache_file
        self.cache = self._load_cache()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'
        }
    
    def _load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f, indent=2)
    
    def scrape_athlete(self, url):
        """ì„ ìˆ˜ ë°ì´í„° í¬ë¡¤ë§"""
        
        # ìºì‹œ í™•ì¸
        if url in self.cache:
            cache_time = datetime.fromisoformat(self.cache[url]['timestamp'])
            if (datetime.now() - cache_time).days < 7:
                print(f"ìºì‹œ ì‚¬ìš©: {url}")
                return self.cache[url]['data']
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ FIS í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
            data = {
                'fis_url': url,
                'fis_code': self._extract_fis_code(url),
                'name_en': self._extract_name(soup),
                'birth_date': self._extract_birth_date(soup),
                'gender': self._extract_gender(soup),
                'current_rank': self._extract_current_rank(soup),
                'best_rank': self._extract_best_rank(soup),
                'season_starts': self._extract_season_starts(soup),
                'medals': self._extract_medals(soup),
                'recent_results': self._extract_recent_results(soup)
            }
            
            # ìºì‹œ ì €ì¥
            self.cache[url] = {
                'timestamp': datetime.now().isoformat(),
                'data': data
            }
            self._save_cache()
            
            # Rate limiting
            time.sleep(2)
            
            return data
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {str(e)}")
            return None
    
    def _extract_fis_code(self, url):
        """URLì—ì„œ FIS ì½”ë“œ ì¶”ì¶œ"""
        # competitorid=123456
        try:
            return url.split('competitorid=')[1]
        except:
            return None
    
    def _extract_name(self, soup):
        """ì´ë¦„ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        try:
            name_elem = soup.find('h1', class_='athlete-name')
            return name_elem.text.strip() if name_elem else None
        except:
            return None
    
    def _extract_birth_date(self, soup):
        """ìƒë…„ì›”ì¼ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        try:
            birth_elem = soup.find('span', class_='birth-date')
            return birth_elem.text.strip() if birth_elem else None
        except:
            return None
    
    def _extract_gender(self, soup):
        """ì„±ë³„ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        return "M"  # ì„ì‹œ
    
    def _extract_current_rank(self, soup):
        """í˜„ì¬ ë­í‚¹ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        return None
    
    def _extract_best_rank(self, soup):
        """ìµœê³  ë­í‚¹ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        return None
    
    def _extract_season_starts(self, soup):
        """ì‹œì¦Œ ì¶œì „ íšŸìˆ˜ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        return 0
    
    def _extract_medals(self, soup):
        """ë©”ë‹¬ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        return {'gold': 0, 'silver': 0, 'bronze': 0}
    
    def _extract_recent_results(self, soup):
        """ìµœê·¼ 5ê²½ê¸° ê²°ê³¼ ì¶”ì¶œ"""
        # ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„
        return []
    
    def scrape_all(self, urls):
        """ì „ì²´ ì„ ìˆ˜ í¬ë¡¤ë§"""
        results = []
        for i, url in enumerate(urls):
            print(f"í¬ë¡¤ë§ ì§„í–‰: {i+1}/{len(urls)}")
            data = self.scrape_athlete(url)
            if data:
                results.append(data)
        return results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    scraper = FISScraper()
    
    # ì„ ìˆ˜ URL ë¦¬ìŠ¤íŠ¸ (ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´ í•„ìš”)
    urls = [
        "https://www.fis-ski.com/DB/general/athlete.html?sectorcode=FS&competitorid=123456",
        # ... 43ëª… ì „ì²´ URL
    ]
    
    results = scraper.scrape_all(urls)
    
    # ê²°ê³¼ ì €ì¥
    with open('data/raw/scraped_data.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ì™„ë£Œ: {len(results)}ëª… ë°ì´í„° ìˆ˜ì§‘")
```

---

## ğŸ”§ Agent B: ë°ì´í„° ì •ì œ

### ì£¼ìš” ì‘ì—…

1. **ì¢…ëª©ëª… í‘œì¤€í™”**
2. **ëˆ„ë½ ë°ì´í„° ì²´í¬**
3. **í†µê³„ ê³„ì‚°**
4. **ìµœì¢… JSON ìƒì„±**

### ì¢…ëª© ë§¤í•‘

```json
{
  "AL": "alpine_skiing",
  "SX": "ski_cross",
  "MO": "freestyle_moguls",
  "FS": "freestyle_park",
  "SB": "snowboard_park",
  "SBX": "snowboard_cross",
  "PSL": "snowboard_alpine"
}
```

### êµ¬í˜„ ì˜ˆì‹œ (Python)

```python
import json
from datetime import datetime
from collections import Counter

class DataProcessor:
    """ë°ì´í„° ì •ì œ ë° í†µê³„ ìƒì„±"""
    
    def __init__(self, input_file="data/raw/scraped_data.json"):
        self.input_file = input_file
        self.sport_mapping = {
            "AL": "alpine_skiing",
            "SX": "ski_cross",
            "MO": "freestyle_moguls",
            "FS": "freestyle_park",
            "SB": "snowboard_park",
            "SBX": "snowboard_cross",
            "PSL": "snowboard_alpine"
        }
        self.sport_display = {
            "alpine_skiing": "Alpine Skiing",
            "ski_cross": "Ski Cross",
            "freestyle_moguls": "Freestyle - Moguls",
            "freestyle_park": "Freestyle - Park & Pipe",
            "snowboard_park": "Snowboard - Park & Pipe",
            "snowboard_cross": "Snowboard Cross",
            "snowboard_alpine": "Snowboard Alpine"
        }
        self.team_mapping = {
            "freestyle_moguls": "í”„ë¦¬ìŠ¤íƒ€ì¼",
            "freestyle_park": "í”„ë¦¬ìŠ¤íƒ€ì¼",
            "snowboard_park": "ìŠ¤ë…¸ë³´ë“œ",
            "snowboard_cross": "ìŠ¤ë…¸ë³´ë“œ",
            "snowboard_alpine": "ìŠ¤ë…¸ë³´ë“œ"
        }
    
    def load_data(self):
        """ì›ë³¸ ë°ì´í„° ë¡œë“œ"""
        with open(self.input_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def calculate_age(self, birth_date):
        """ë‚˜ì´ ê³„ì‚°"""
        if not birth_date:
            return None
        try:
            birth = datetime.strptime(birth_date, "%Y-%m-%d")
            today = datetime.now()
            return today.year - birth.year - ((today.month, today.day) < (birth.month, birth.day))
        except:
            return None
    
    def standardize_sport(self, sport_code):
        """ì¢…ëª© í‘œì¤€í™”"""
        return self.sport_mapping.get(sport_code, "unknown")
    
    def process_athletes(self, raw_data):
        """ì„ ìˆ˜ ë°ì´í„° ì²˜ë¦¬"""
        processed = []
        
        for i, athlete in enumerate(raw_data):
            sport = self.standardize_sport(athlete.get('sport_code', ''))
            
            processed_athlete = {
                'id': f"KOR{i+1:03d}",
                'name_ko': athlete.get('name_ko', ''),
                'name_en': athlete.get('name_en', ''),
                'birth_date': athlete.get('birth_date'),
                'age': self.calculate_age(athlete.get('birth_date')),
                'gender': athlete.get('gender', 'M'),
                'sport': sport,
                'sport_display': self.sport_display.get(sport, ''),
                'team': self.team_mapping.get(sport, 'ê¸°íƒ€'),
                'fis_code': athlete.get('fis_code'),
                'fis_url': athlete.get('fis_url'),
                'current_rank': athlete.get('current_rank'),
                'best_rank': athlete.get('best_rank'),
                'season_starts': athlete.get('season_starts', 0),
                'medals': athlete.get('medals', {'gold': 0, 'silver': 0, 'bronze': 0}),
                'recent_results': athlete.get('recent_results', [])
            }
            
            processed.append(processed_athlete)
        
        return processed
    
    def generate_statistics(self, athletes):
        """í†µê³„ ìƒì„±"""
        stats = {
            'total_athletes': len(athletes),
            'by_sport': Counter(a['sport'] for a in athletes),
            'by_team': Counter(a['team'] for a in athletes),
            'by_gender': Counter(a['gender'] for a in athletes),
            'age_distribution': {
                'teens': sum(1 for a in athletes if a['age'] and 10 <= a['age'] < 20),
                'twenties': sum(1 for a in athletes if a['age'] and 20 <= a['age'] < 30),
                'thirties': sum(1 for a in athletes if a['age'] and 30 <= a['age'] < 40),
            },
            'total_medals': {
                'gold': sum(a['medals']['gold'] for a in athletes),
                'silver': sum(a['medals']['silver'] for a in athletes),
                'bronze': sum(a['medals']['bronze'] for a in athletes)
            }
        }
        return stats
    
    def validate_data(self, athletes):
        """ë°ì´í„° ê²€ì¦"""
        issues = []
        
        for athlete in athletes:
            # í•„ìˆ˜ í•„ë“œ ì²´í¬
            if not athlete.get('name_en'):
                issues.append(f"{athlete['id']}: ì˜ë¬¸ ì´ë¦„ ëˆ„ë½")
            if not athlete.get('fis_code'):
                issues.append(f"{athlete['id']}: FIS ì½”ë“œ ëˆ„ë½")
            if not athlete.get('birth_date'):
                issues.append(f"{athlete['id']}: ìƒë…„ì›”ì¼ ëˆ„ë½")
        
        return issues
    
    def save_final_data(self, athletes, output_file="data/athletes.json"):
        """ìµœì¢… JSON ì €ì¥"""
        stats = self.generate_statistics(athletes)
        
        final_data = {
            'metadata': {
                'last_updated': datetime.now().isoformat(),
                'total_athletes': stats['total_athletes'],
                'sports': len(stats['by_sport']),
                'teams': len(stats['by_team'])
            },
            'statistics': stats,
            'athletes': athletes
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   - ì´ ì„ ìˆ˜: {stats['total_athletes']}ëª…")
        print(f"   - ì¢…ëª©: {stats['by_sport']}")
        print(f"   - íŒ€: {stats['by_team']}")
    
    def process(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ë°ì´í„° ë¡œë“œ
        raw_data = self.load_data()
        print(f"âœ“ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(raw_data)}ê±´")
        
        # 2. ë°ì´í„° ì²˜ë¦¬
        athletes = self.process_athletes(raw_data)
        print(f"âœ“ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(athletes)}ëª…")
        
        # 3. ë°ì´í„° ê²€ì¦
        issues = self.validate_data(athletes)
        if issues:
            print("âš ï¸  ë°ì´í„° ê²€ì¦ ì´ìŠˆ:")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print("âœ“ ë°ì´í„° ê²€ì¦ í†µê³¼")
        
        # 4. ìµœì¢… ì €ì¥
        self.save_final_data(athletes)
        
        return athletes


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    processor = DataProcessor()
    athletes = processor.process()
```

---

## ğŸ“… ìë™í™” ìŠ¤ì¼€ì¤„ëŸ¬

### Cron ì„¤ì •

```bash
# ë§¤ì£¼ ìˆ˜ìš”ì¼ ì˜¤ì „ 9ì‹œ ì‹¤í–‰
0 9 * * 3 cd /path/to/project && python src/data_pipeline.py
```

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```python
#!/usr/bin/env python3
"""
data_pipeline.py
ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.fis_scraper import FISScraper
from src.data_processor import DataProcessor

def main():
    print("=" * 50)
    print("Team Korea Data Pipeline")
    print("=" * 50)
    
    # 1. ì„ ìˆ˜ URL ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    url_file = project_root / "data" / "raw" / "athlete_urls.txt"
    with open(url_file, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]
    
    print(f"\nğŸ“‹ ì„ ìˆ˜ URL: {len(urls)}ê°œ")
    
    # 2. FIS ë°ì´í„° í¬ë¡¤ë§
    print("\nğŸ” FIS ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
    scraper = FISScraper()
    raw_data = scraper.scrape_all(urls)
    print(f"âœ“ í¬ë¡¤ë§ ì™„ë£Œ: {len(raw_data)}ëª…")
    
    # 3. ë°ì´í„° ì²˜ë¦¬
    print("\nğŸ“Š ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
    processor = DataProcessor()
    athletes = processor.process()
    
    print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 50)

if __name__ == "__main__":
    main()
```

---

## ğŸ“‚ ë°ì´í„° íŒŒì¼ êµ¬ì¡°

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ athlete_urls.txt        # ì„ ìˆ˜ URL ë¦¬ìŠ¤íŠ¸
â”‚   â””â”€â”€ scraped_data.json       # í¬ë¡¤ë§ ì›ë³¸
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ validated_data.json     # ê²€ì¦ëœ ë°ì´í„°
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ scraper_cache.json      # í¬ë¡¤ë§ ìºì‹œ
â””â”€â”€ athletes.json               # ìµœì¢… ë°ì´í„° (ë°°í¬ìš©)
```

---

## ğŸ”” ì—ëŸ¬ ì²˜ë¦¬

### ì¼ë°˜ì ì¸ ì—ëŸ¬

1. **ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬**: ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
2. **íŒŒì‹± ì—ëŸ¬**: ë¡œê·¸ ê¸°ë¡, í•´ë‹¹ ì„ ìˆ˜ ìŠ¤í‚µ
3. **ì¸ì½”ë”© ì—ëŸ¬**: UTF-8 ê°•ì œ ì ìš©
4. **Rate Limit**: 2ì´ˆ ì§€ì—°

### ë¡œê¹…

```python
import logging

logging.basicConfig(
    filename='logs/data_pipeline.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì„ ìˆ˜ URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì‹  (43ëª…)
- [ ] FIS ìŠ¤í¬ë˜í¼ ê°œë°œ
- [ ] ë°ì´í„° ì •ì œ ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ
- [ ] athletes.json ìƒì„±
- [ ] ë°ì´í„° ê²€ì¦ (ëˆ„ë½ ì²´í¬)
- [ ] Cron ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ êµ¬í˜„
- [ ] ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•

---

**ë‹´ë‹¹ì**: Data Team Agents  
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026-01-23  
**ìƒíƒœ**: ğŸŸ¡ ì„ ìˆ˜ URL ëŒ€ê¸° ì¤‘
