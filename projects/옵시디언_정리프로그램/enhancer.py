#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Obsidian Vault Organizer - Enhancement Patch
ê¸°ì¡´ ë¦¬í¬íŠ¸ì— ì—°ê²° ê°•í™”, MOC, ê´€ê³„ ì‹œê°í™”, ìŠ¤ë§ˆíŠ¸ í•„í„° ì¶”ê°€
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter
import re

class ReportEnhancer:
    def __init__(self, output_path):
        self.output_path = Path(output_path)
        self.snapshot = self.load_latest_snapshot()
        self.files_data = self.snapshot['files']
        
    def load_latest_snapshot(self):
        """ìµœì‹  ìŠ¤ëƒ…ìƒ· ë¡œë“œ"""
        history_dir = self.output_path / "history"
        snapshots = list(history_dir.glob("snapshot_*.json"))
        if not snapshots:
            print("âš ï¸  ìŠ¤ëƒ…ìƒ· íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. main.pyê°€ ë¨¼ì € ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return {'files': []}
        latest = max(snapshots, key=lambda x: x.stat().st_mtime)
        
        with open(latest, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def add_navigation_links(self):
        """ëª¨ë“  ë¦¬í¬íŠ¸ ìƒë‹¨ì— ë‚´ë¹„ê²Œì´ì…˜ ì¶”ê°€"""
        nav = """---
**ğŸ“ Quick Nav:** [[00_Master_Index|ğŸ  Home]] | [[01_Categories|ğŸ“‚ Categories]] | [[02_Tags_System|ğŸ·ï¸ Tags]] | [[03_Action_Queue|ğŸ”§ Actions]] | [[04_Statistics|ğŸ“Š Stats]] | [[05_Smart_Filters|âš¡ Filters]] | [[06_Frequent_Phrases|ğŸ’¬ Phrases]]
---

"""
        
        for report in ['00_Master_Index.md', '01_Categories.md', '02_Tags_System.md', 
                       '03_Action_Queue.md', '04_Statistics.md']:
            file_path = self.output_path / report
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ê¸°ì¡´ nav ì œê±° í›„ ìƒˆë¡œ ì¶”ê°€
                if '**ğŸ“ Quick Nav:**' in content:
                    content = re.sub(r'---\n\*\*ğŸ“ Quick Nav:.*?---\n\n', '', content, flags=re.DOTALL)
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(nav + content)
        
        print("âœ“ ë‚´ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ê°€ ì™„ë£Œ")
    
    def create_moc_structure(self):
        """ì£¼ì œë³„ MOC (Map of Content) ìƒì„±"""
        output = []
        output.append("# ğŸ—ºï¸ Map of Contents - ì£¼ì œë³„ í—ˆë¸Œ\n")
        output.append("## ğŸ”ï¸ ê²¨ìš¸ ìŠ¤í¬ì¸  í—ˆë¸Œ\n")
        
        sports_files = [f for f in self.files_data if f['category'] == 'Winter Sports']
        if sports_files:
            # FIS ê´€ë ¨
            fis_files = [f for f in sports_files if 'fis' in f['title'].lower() or 'mogul' in f['title'].lower()]
            output.append("### FIS & Moguls Coverage\n")
            for f in sorted(fis_files, key=lambda x: x['word_count'], reverse=True)[:10]:
                output.append(f"- [[{f['title']}]] ({f['word_count']} words)")
            
            # UPSHOT ê´€ë ¨
            upshot_files = [f for f in sports_files if 'upshot' in f['title'].lower()]
            if upshot_files:
                output.append("\n### UPSHOT Newsletter Archive\n")
                for f in sorted(upshot_files, key=lambda x: x['last_modified'], reverse=True):
                    output.append(f"- [[{f['title']}]] - {f['last_modified']}")
        
        output.append("\n## ğŸ—£ï¸ í†µì—­ & ì–¸ì–´ í•™ìŠµ í—ˆë¸Œ\n")
        lang_files = [f for f in self.files_data if f['category'] == 'Language & Interpretation']
        if lang_files:
            # í‘œí˜„ ê´€ë ¨
            expr_files = [f for f in lang_files if 'í‘œí˜„' in f['title']]
            if expr_files:
                output.append("### í‘œí˜„ ë°ì´í„°ë² ì´ìŠ¤\n")
                for f in expr_files:
                    output.append(f"- [[{f['title']}]] ({f['word_count']} words)")
            
            # í†µëŒ€ ê´€ë ¨
            tugs_files = [f for f in lang_files if 'í†µëŒ€' in f['title'] or 'í†µì—­' in f['title']]
            if tugs_files:
                output.append("\n### í†µëŒ€ ì¤€ë¹„ & í†µì—­ ë…¸íŠ¸\n")
                for f in sorted(tugs_files, key=lambda x: x['word_count'], reverse=True)[:10]:
                    output.append(f"- [[{f['title']}]] ({f['word_count']} words)")
        
        output.append("\n## ğŸ¤– AI & Tools í—ˆë¸Œ\n")
        ai_files = [f for f in self.files_data if f['category'] == 'AI & Tools']
        if ai_files:
            for f in sorted(ai_files, key=lambda x: x['word_count'], reverse=True)[:10]:
                output.append(f"- [[{f['title']}]] ({f['word_count']} words)")
        
        output.append("\n## ğŸ“° ë¯¸ë””ì–´ & ë‰´ìŠ¤ë ˆí„° í—ˆë¸Œ\n")
        media_files = [f for f in self.files_data if f['category'] == 'Media & Newsletter']
        if media_files:
            for f in sorted(media_files, key=lambda x: x['last_modified'], reverse=True):
                output.append(f"- [[{f['title']}]] - {f['last_modified']}")
        
        self.save_file("07_MOC_Hub.md", '\n'.join(output))
    
    def create_relationship_diagram(self):
        """ê´€ê³„ ì‹œê°í™” ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±"""
        output = []
        output.append("# ğŸ”— ë¬¸ì„œ ê´€ê³„ ì‹œê°í™”\n")
        
        if not self.files_data:
            output.append("âš ï¸ ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            self.save_file("08_Relationship_Map.md", '\n'.join(output))
            return
        
        # í†µê³„ ì •ë³´
        total_files = len(self.files_data)
        total_links = sum(f['links_out'] for f in self.files_data)
        output.append(f"**ì´ ë¬¸ì„œ ìˆ˜**: {total_files} | **ì´ ë§í¬ ìˆ˜**: {total_links}\n")
        
        # 1. ë§í¬ê°€ ë§ì€ í—ˆë¸Œ ë¬¸ì„œë“¤ (incoming links ê¸°ì¤€)
        output.append("## ğŸ“Š ì£¼ìš” í—ˆë¸Œ ë¬¸ì„œ (Incoming Links)\n")
        hubs_by_incoming = sorted(self.files_data, key=lambda x: x['links_in'], reverse=True)[:10]
        
        if hubs_by_incoming and hubs_by_incoming[0]['links_in'] > 0:
            for i, hub in enumerate(hubs_by_incoming, 1):
                output.append(f"{i}. **[[{hub['title']}]]** - {hub['links_in']} ê°œ ë¬¸ì„œì—ì„œ ì°¸ì¡°ë¨")
        else:
            output.append("*í˜„ì¬ incoming linksê°€ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.*\n")
        
        # 2. ë§í¬ë¥¼ ë§ì´ í•˜ëŠ” ë¬¸ì„œë“¤ (outgoing links ê¸°ì¤€)
        output.append("\n## ğŸ”— ì—°ê²°ì´ ë§ì€ ë¬¸ì„œ (Outgoing Links)\n")
        hubs_by_outgoing = sorted(self.files_data, key=lambda x: x['links_out'], reverse=True)[:10]
        
        if hubs_by_outgoing and hubs_by_outgoing[0]['links_out'] > 0:
            for i, hub in enumerate(hubs_by_outgoing, 1):
                output.append(f"{i}. **[[{hub['title']}]]** - {hub['links_out']} ê°œ ë¬¸ì„œë¡œ ë§í¬")
        else:
            output.append("*í˜„ì¬ outgoing linksê°€ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.*\n")
        
        # 3. ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê´€ê³„ë„ ìƒì„±
        output.append("\n## ğŸ—ºï¸ ë¬¸ì„œ ì—°ê²° ë‹¤ì´ì–´ê·¸ë¨\n")
        
        # ìƒìœ„ 5ê°œ í—ˆë¸Œ ë¬¸ì„œì˜ ì‹¤ì œ ì—°ê²° ê´€ê³„ ì‹œê°í™”
        top_hubs = sorted(self.files_data, key=lambda x: x['links_out'], reverse=True)[:5]
        
        if top_hubs and top_hubs[0]['links_out'] > 0:
            output.append("```mermaid")
            output.append("graph TD")
            
            # ë…¸ë“œ IDë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
            def safe_id(text, prefix="N"):
                # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
                safe = re.sub(r'[^a-zA-Z0-9ê°€-í£]', '', text)
                return f"{prefix}_{safe[:15]}"
            
            # ê° í—ˆë¸Œ ë¬¸ì„œì™€ ê·¸ ì—°ê²°ë“¤
            for i, hub in enumerate(top_hubs, 1):
                hub_id = safe_id(hub['title'], f"Hub{i}")
                hub_label = hub['title'][:30].replace('"', "'")
                
                output.append(f'    {hub_id}["{hub_label}"]')
                
                # ì´ ë¬¸ì„œê°€ ë§í¬í•˜ëŠ” ë¬¸ì„œë“¤ (ìµœëŒ€ 5ê°œ)
                for j, link in enumerate(hub['links_out_list'][:5], 1):
                    # ë§í¬ í…ìŠ¤íŠ¸ ì •ë¦¬ (íŒŒì´í”„ ê¸°í˜¸ë¡œ ë¶„ë¦¬ëœ ê²½ìš° ì²« ë¶€ë¶„ë§Œ ì‚¬ìš©)
                    link_text = link.split('|')[0][:25].replace('"', "'")
                    link_id = safe_id(link_text, f"L{i}_{j}")
                    
                    output.append(f'    {hub_id} --> {link_id}["{link_text}"]')
            
            output.append("```\n")
        else:
            output.append("*í˜„ì¬ ë§í¬ ê´€ê³„ë¥¼ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.*\n")
        
        # 4. ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„í¬
        output.append("## ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„í¬\n")
        
        category_counts = Counter(f['category'] for f in self.files_data)
        
        if category_counts:
            output.append("```mermaid")
            output.append("pie title ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„í¬")
            for category, count in category_counts.most_common():
                safe_category = category.replace('"', "'")
                output.append(f'    "{safe_category}" : {count}')
            output.append("```\n")
        
        # 5. ì¹´í…Œê³ ë¦¬ ê°„ ì—°ê²° êµ¬ì¡° (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
        output.append("## ğŸŒ ì¹´í…Œê³ ë¦¬ ê°„ ì—°ê²° êµ¬ì¡°\n")
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ê°€ì¥ í™œë°œí•œ ë¬¸ì„œ ì°¾ê¸°
        category_hubs = {}
        for category in category_counts.keys():
            cat_files = [f for f in self.files_data if f['category'] == category]
            if cat_files:
                top_file = max(cat_files, key=lambda x: x['links_out'])
                if top_file['links_out'] > 0:
                    category_hubs[category] = top_file
        
        if category_hubs:
            output.append("```mermaid")
            output.append("graph LR")
            
            for i, (category, hub) in enumerate(category_hubs.items(), 1):
                cat_id = safe_id(category, "Cat")
                cat_label = category[:20].replace('"', "'")
                output.append(f'    {cat_id}["{cat_label}<br/>{hub["links_out"]} links"]')
            
            output.append("```\n")
        else:
            output.append("*ì¹´í…Œê³ ë¦¬ ê°„ ì—°ê²° êµ¬ì¡°ë¥¼ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.*\n")
        
        # 6. ìƒì„¸ ë§í¬ ëª©ë¡ (ë””ë²„ê¹…ìš©)
        output.append("## ğŸ“‹ ìƒì„¸ ë§í¬ ë¶„ì„\n")
        for i, hub in enumerate(hubs_by_outgoing[:3], 1):
            output.append(f"\n### {i}. [[{hub['title']}]]\n")
            output.append(f"- **ì¹´í…Œê³ ë¦¬**: {hub['category']}")
            output.append(f"- **ë‹¨ì–´ ìˆ˜**: {hub['word_count']}")
            output.append(f"- **Outgoing Links**: {hub['links_out']}")
            output.append(f"- **Incoming Links**: {hub['links_in']}")
            output.append(f"- **ìµœì¢… ìˆ˜ì •**: {hub['last_modified']}\n")
            
            if hub['links_out_list']:
                output.append("**ë§í¬ ëª©ë¡** (ì²˜ìŒ 10ê°œ):")
                for link in hub['links_out_list'][:10]:
                    link_display = link.split('|')[0][:50]
                    output.append(f"  - {link_display}")
                if len(hub['links_out_list']) > 10:
                    output.append(f"  - ... ì™¸ {len(hub['links_out_list']) - 10}ê°œ")
        
        self.save_file("08_Relationship_Map.md", '\n'.join(output))
    
    def create_smart_filters(self):
        """ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ - ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ë¶„ë¥˜"""
        output = []
        output.append("# âš¡ Smart Filters - ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ í•„í„°\n")
        
        # 1. ì›Œë“œ ì¹´ìš´íŠ¸ ê¸°ì¤€
        output.append("## ğŸ“ ì›Œë“œ ì¹´ìš´íŠ¸ë³„ ë¶„ë¥˜\n")
        
        output.append("### ğŸ”¥ Long Form (2000+ words)\n")
        long_docs = [f for f in self.files_data if f['word_count'] >= 2000]
        for f in sorted(long_docs, key=lambda x: x['word_count'], reverse=True)[:15]:
            output.append(f"- [[{f['title']}]] - {f['word_count']} words")
        
        output.append("\n### ğŸ“ Medium (500-2000 words)\n")
        medium_docs = [f for f in self.files_data if 500 <= f['word_count'] < 2000]
        for f in sorted(medium_docs, key=lambda x: x['word_count'], reverse=True)[:15]:
            output.append(f"- [[{f['title']}]] - {f['word_count']} words")
        
        output.append("\n### ğŸ’¡ Short Notes (100-500 words)\n")
        short_docs = [f for f in self.files_data if 100 <= f['word_count'] < 500]
        for f in sorted(short_docs, key=lambda x: x['word_count'], reverse=True)[:15]:
            output.append(f"- [[{f['title']}]] - {f['word_count']} words")
        
        output.append("\n### âš ï¸ Stub (10-100 words) - í™•ì¥ í•„ìš”\n")
        stub_docs = [f for f in self.files_data if 10 <= f['word_count'] < 100]
        for f in sorted(stub_docs, key=lambda x: x['word_count'], reverse=True)[:20]:
            output.append(f"- [ ] [[{f['title']}]] - {f['word_count']} words")
        
        # 2. ìµœê·¼ í™œë™ ê¸°ì¤€
        output.append("\n## ğŸ“… ìµœê·¼ í™œë™ ê¸°ì¤€\n")
        output.append("### ğŸ”¥ ì´ë²ˆ ë‹¬ ì‘ì—… (ìµœê·¼ 30ì¼)\n")
        recent = sorted(self.files_data, key=lambda x: x['last_modified'], reverse=True)[:20]
        for f in recent:
            output.append(f"- [[{f['title']}]] - {f['last_modified']} ({f['word_count']} words)")
        
        # 3. ì™„ì„±ë„ ê¸°ì¤€
        output.append("\n## âœ… ì™„ì„±ë„ ê¸°ì¤€\n")
        output.append("### ğŸŒŸ High Quality (500+ words, 2+ links)\n")
        quality = [f for f in self.files_data 
                   if f['word_count'] >= 500 and f['links_out'] >= 2]
        for f in sorted(quality, key=lambda x: x['word_count'], reverse=True)[:20]:
            output.append(f"- [[{f['title']}]] - {f['word_count']} words, {f['links_out']} links")
        
        output.append("\n### ğŸš§ Work in Progress (100-500 words, few links)\n")
        wip = [f for f in self.files_data 
               if 100 <= f['word_count'] < 500 and f['links_out'] < 2]
        for f in sorted(wip, key=lambda x: x['last_modified'], reverse=True)[:20]:
            output.append(f"- [ ] [[{f['title']}]] - {f['word_count']} words")
        
        self.save_file("05_Smart_Filters.md", '\n'.join(output))
    
    def create_frequent_phrases(self):
        """ìì£¼ ì“°ëŠ” í‘œí˜„ ë¶„ì„ (íƒœê·¸ ëŒ€ì‹ )"""
        output = []
        output.append("# ğŸ’¬ ìì£¼ ì“°ëŠ” í‘œí˜„ & í‚¤ì›Œë“œ ë¶„ì„\n")
        
        # ëª¨ë“  íŒŒì¼ì˜ ì œëª©ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        all_words = []
        for f in self.files_data:
            # í•œê¸€, ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', f['title'])
            all_words.extend(words)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['the', 'and', 'for', 'with', 'this', 'that', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ìˆë‹¤', 'í•œë‹¤']
        all_words = [w for w in all_words if w.lower() not in stopwords]
        
        word_counts = Counter(all_words)
        
        output.append("## ğŸ“Š ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ TOP 30\n")
        for word, count in word_counts.most_common(30):
            # í•´ë‹¹ ë‹¨ì–´ í¬í•¨ ë¬¸ì„œ ì°¾ê¸°
            docs_with_word = [f for f in self.files_data if word in f['title']]
            output.append(f"### {word} ({count}íšŒ)\n")
            for doc in docs_with_word[:5]:
                output.append(f"- [[{doc['title']}]]")
            if len(docs_with_word) > 5:
                output.append(f"- ... ì™¸ {len(docs_with_word)-5}ê°œ")
            output.append("")
        
        # ì˜ì–´ í‘œí˜„ vs í•œê¸€ í‘œí˜„
        korean_words = [w for w in all_words if re.match(r'[ê°€-í£]+', w)]
        english_words = [w for w in all_words if re.match(r'[a-zA-Z]+', w)]
        
        output.append("\n## ğŸ‡°ğŸ‡· í•œê¸€ í‚¤ì›Œë“œ TOP 10\n")
        for word, count in Counter(korean_words).most_common(10):
            output.append(f"- **{word}**: {count}íšŒ")
        
        output.append("\n## ğŸ‡ºğŸ‡¸ ì˜ì–´ í‚¤ì›Œë“œ TOP 10\n")
        for word, count in Counter(english_words).most_common(10):
            output.append(f"- **{word}**: {count}íšŒ")
        
        self.save_file("06_Frequent_Phrases.md", '\n'.join(output))
    
    def save_file(self, filename, content):
        """íŒŒì¼ ì €ì¥"""
        filepath = self.output_path / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ“ {filename} ìƒì„±")


def main():
    print("\n" + "="*60)
    print("ğŸš€ Obsidian Vault Organizer - Enhancement Patch")
    print("="*60 + "\n")
    
    output_path = "/Users/jenkim/Library/Mobile Documents/iCloud~md~obsidian/Documents/Mac book ì˜µì‹œë””ì–¸/output"
    
    try:
        enhancer = ReportEnhancer(output_path)
        
        print("ğŸ“ ê³ ë„í™” ê¸°ëŠ¥ ì¶”ê°€ ì¤‘...\n")
        enhancer.add_navigation_links()
        enhancer.create_moc_structure()
        enhancer.create_relationship_diagram()
        enhancer.create_smart_filters()
        enhancer.create_frequent_phrases()
        
        print("\n" + "="*60)
        print("âœ… ëª¨ë“  ê³ ë„í™” ì™„ë£Œ!")
        print("="*60)
        print("ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼:")
        print("  - 05_Smart_Filters.md")
        print("  - 06_Frequent_Phrases.md")
        print("  - 07_MOC_Hub.md")
        print("  - 08_Relationship_Map.md")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
