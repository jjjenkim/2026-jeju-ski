#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Obsidian Vault Organizer - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¸ë²¤í† ë¦¬ ì‹œìŠ¤í…œ
í† í° ì ˆì•½í˜• ì„¤ê³„: AI ì—†ì´ íŒŒì¼ ë©”íƒ€ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²´ ì§€ë„ ìƒì„±
"""

import os
import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter

class VaultAnalyzer:
    def __init__(self, vault_path, output_path):
        self.vault_path = Path(vault_path)
        self.output_path = Path(output_path)
        self.files_data = []
        self.stats = defaultdict(int)
        
    def scan_vault(self):
        """ëª¨ë“  .md íŒŒì¼ ìŠ¤ìº” (ë©”íƒ€ë°ì´í„°ë§Œ)"""
        print("\nğŸ“‚ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        md_files = list(self.vault_path.rglob("*.md"))
        
        # _Inventoryì™€ output í´ë”ëŠ” ì œì™¸
        md_files = [f for f in md_files if "_Inventory" not in str(f) and "output" not in str(f)]
        
        print(f"âœ“ {len(md_files)}ê°œì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë°œê²¬\n")
        
        for i, file_path in enumerate(md_files, 1):
            print(f"\rì§„í–‰: {i}/{len(md_files)} ({i/len(md_files)*100:.1f}%)", end="")
            metadata = self.extract_metadata(file_path)
            # ë¹ˆ íŒŒì¼ ì œì™¸ (10 words ë¯¸ë§Œ)
            if metadata['word_count'] >= 10:
                self.files_data.append(metadata)
        
        print("\nâœ“ ìŠ¤ìº” ì™„ë£Œ\n")
        return self.files_data
    
    def extract_metadata(self, file_path):
        """íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (AI ì—†ì´)"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                first_100_lines = '\n'.join(content.split('\n')[:100])
        except:
            content = ""
            first_100_lines = ""
        
        # ê¸°ë³¸ ì •ë³´
        stat = file_path.stat()
        
        # ì œëª©: íŒŒì¼ëª… ìš°ì„  ì‚¬ìš© (Daily Notes êµ¬ë¶„ ìœ„í•´)
        title = file_path.stem
        
        # íƒœê·¸ ì¶”ì¶œ
        tags = re.findall(r'#(\w+)', first_100_lines)
        tags = list(set(tags))[:5]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 5ê°œ
        
        # ë§í¬ ì¶”ì¶œ
        links_out = re.findall(r'\[\[([^\]]+)\]\]', content)
        
        # ë‹¨ì–´ ìˆ˜
        word_count = len(content.split())
        
        # ë¶„ë¥˜ ì¶”ì •
        category = self.estimate_category(file_path.name, content, tags)
        
        # ì‹ ë¢°ë„
        confidence = self.estimate_confidence(word_count, len(links_out))
        
        return {
            'path': str(file_path.relative_to(self.vault_path)),
            'name': file_path.name,
            'title': title,
            'category': category,
            'tags': tags,
            'word_count': word_count,
            'links_out': len(links_out),
            'links_out_list': links_out[:10],  # ìµœëŒ€ 10ê°œë§Œ
            'confidence': confidence,
            'last_modified': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d'),
            'size_bytes': stat.st_size
        }

    def estimate_category(self, filename, content, tags):
        """ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì •"""
        filename_lower = filename.lower()
        content_lower = content[:1000].lower()
        tags_str = ' '.join(tags).lower()
        
        # ğŸ¤– AI & Tools
        ai_keywords = ['gpt', 'chatgpt', 'claude', 'perplexity', 'ai', 'í”„ë¡¬í”„íŠ¸', 'prompt', 
                       'gemini', 'anthropic', 'llm', 'notebooklm', 'ìë™í™”', 'automation']
        if any(word in filename_lower or word in content_lower or word in tags_str 
               for word in ai_keywords):
            return 'AI & Tools'
        
        # ğŸ—£ï¸ í†µì—­ & ì–¸ì–´
        interp_keywords = ['í†µì—­', 'ìˆœì°¨í†µì—­', 'ë™ì‹œí†µì—­', 'interpretation', 'interpreter', 
                           'í‘œí˜„', 'ì˜ì–´', 'english', 'í†µëŒ€', 'ì–¸ì–´', 'ë°œìŒ', 'ë¬¸ë²•', 
                           'broca', 'note-taking', 'ë©”ëª¨ë¦¬', 'êµ¬ìˆ ', 'ë²ˆì—­']
        if any(word in filename_lower or word in content_lower or word in tags_str 
               for word in interp_keywords):
            return 'Language & Interpretation'
        
        # ğŸ”ï¸ ìŠ¤í¬ì¸ 
        sports_keywords = ['fis', 'mogul', 'freestyle', 'ìŠ¤í‚¤', 'ìŠ¤ë…¸ë³´ë“œ', 'halfpipe', 
                           'ì˜¬ë¦¼í”½', 'olympic', 'world cup', 'upshot', 'ì„ ìˆ˜', 'athlete',
                           'ì›”ë“œì»µ', 'í•˜í”„íŒŒì´í”„', 'ëª¨ê¸€']
        if any(word in filename_lower or word in content_lower or word in tags_str 
               for word in sports_keywords):
            return 'Winter Sports'
        
        # ğŸ“° ë¯¸ë””ì–´ & ë‰´ìŠ¤ë ˆí„°
        media_keywords = ['ë‰´ìŠ¤ë ˆí„°', 'newsletter', 'ë©”ì¼ë¦¬', 'upshot', 'template', 
                          'seo', 'ì½˜í…ì¸ ', 'content', 'ë°œí–‰', 'êµ¬ë…', 'cover story']
        if any(word in filename_lower or word in content_lower or word in tags_str 
               for word in media_keywords):
            return 'Media & Newsletter'
        
        # ğŸ“ í•™ì—… & ì—°êµ¬
        academic_keywords = ['ë…¼ë¬¸', 'thesis', 'ì—°êµ¬', 'research', 'í•™ìŠµ', 'ê³µë¶€', 
                            'ìŠ¤í„°ë””', 'study', 'ì™¸ëŒ€', 'í†µë²ˆì—­', 'ìˆ˜ì—…', 'diplomacy']
        if any(word in filename_lower or word in content_lower or word in tags_str 
               for word in academic_keywords):
            return 'Academic & Research'
        
        # ğŸ“ Notes (ì¼ì¼ ë©”ëª¨)
        if any(word in filename_lower 
               for word in ['today', 'ë©”ëª¨', 'note', 'ì„ì‹œ', 'temp', 'ì´ˆì•ˆ', 'draft', 'ìƒê°']):
            return 'Notes'
        
        # ğŸš€ Projects
        if any(word in filename_lower or word in content_lower 
               for word in ['í”„ë¡œì íŠ¸', 'project', 'ì§„í–‰', 'todo', 'task', 'mvp', 'êµ¬í˜„']):
            return 'Projects'
        
        # ğŸ”— Resources
        if any(word in filename_lower or word in content_lower 
               for word in ['ë§í¬', 'link', 'ìë£Œ', 'resource', 'reference', 'ê°€ì´ë“œ', 'guide']):
            return 'Resources'
        
        # ê¸°ë³¸ê°’
        return 'General Knowledge'


    
    def estimate_confidence(self, word_count, link_count):
        """ë¬¸ì„œ ì™„ì„±ë„ ì¶”ì •"""
        if word_count < 50:
            return 'draft'
        elif word_count < 300:
            return 'partial'
        else:
            return 'complete'
    
    def calculate_links_in(self):
        """ì—­ë§í¬ ê³„ì‚°"""
        links_in_map = defaultdict(int)
        
        for file_data in self.files_data:
            for link in file_data['links_out_list']:
                # ë§í¬ì—ì„œ í™•ì¥ì ì œê±°
                link_clean = link.replace('.md', '')
                links_in_map[link_clean] += 1
        
        # ê° íŒŒì¼ì— links_in ì¶”ê°€
        for file_data in self.files_data:
            file_title = file_data['title']
            file_data['links_in'] = links_in_map.get(file_title, 0)
    
    def generate_reports(self):
        """ëª¨ë“  ë¦¬í¬íŠ¸ ìƒì„±"""
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...\n")
        
        # ì—­ë§í¬ ê³„ì‚°
        self.calculate_links_in()
        
        # ê°ì¢… ì¸ë±ìŠ¤ ìƒì„±
        self.create_master_index()
        self.create_categories_index()
        self.create_tags_index()
        self.create_action_queue()
        self.create_statistics()
        
        # ìŠ¤ëƒ…ìƒ· ì €ì¥
        self.save_snapshot()
        
        print("âœ“ ëª¨ë“  ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ\n")
    
    def create_master_index(self):
        """00_Master_Index.md ìƒì„±"""
        output = []
        output.append("# ğŸ“Š Vault Master Index")
        output.append(f"> ìµœì¢… ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        
        # í†µê³„
        total = len(self.files_data)
        by_category = Counter(f['category'] for f in self.files_data)
        
        output.append("## ğŸ“ˆ í†µê³„ ìš”ì•½")
        output.append(f"- ì´ ë¬¸ì„œ: {total}ê°œ")
        for cat, count in by_category.most_common():
            emoji_map = {
                'AI & Tools': 'ğŸ¤–',
                'Language & Interpretation': 'ğŸ—£ï¸',
                'Winter Sports': 'ğŸ”ï¸',
                'Media & Newsletter': 'ğŸ“°',
                'Academic & Research': 'ğŸ“',
                'Projects': 'ğŸš€',
                'Notes': 'ğŸ“',
                'Resources': 'ğŸ”—',
                'General Knowledge': 'ğŸ“š'
            }
            emoji = emoji_map.get(cat, 'ğŸ“„')
            output.append(f"- {emoji} {cat}: {count}ê°œ")
        
        # ìµœê·¼ ìˆ˜ì •
        output.append("\n## ğŸ“Œ ìµœê·¼ ìˆ˜ì • (7ì¼ ì´ë‚´)")
        recent = sorted(self.files_data, key=lambda x: x['last_modified'], reverse=True)[:10]
        for f in recent:
            output.append(f"- [[{f['title']}]] - {f['last_modified']} ({f['word_count']} words)")
        
        # í—ˆë¸Œ ë¬¸ì„œ
        output.append("\n## ğŸ”¥ í—ˆë¸Œ ë¬¸ì„œ (ë§í¬ ë§ìŒ)")
        hubs = sorted(self.files_data, key=lambda x: x['links_in'], reverse=True)[:10]
        for f in hubs:
            if f['links_in'] > 0:
                output.append(f"- [[{f['title']}]] ({f['links_in']}ê°œ ë§í¬)")
        
        # ê³ ì•„ ë¬¸ì„œ
        orphans = [f for f in self.files_data if f['links_in'] == 0 and f['links_out'] == 0]
        output.append(f"\n## âš ï¸ ê³ ì•„ ë¬¸ì„œ (ë§í¬ ì—†ìŒ): {len(orphans)}ê°œ")
        for f in orphans[:10]:
            output.append(f"- [[{f['title']}]] ({f['word_count']} words)")
        
        self.save_file("00_Master_Index.md", '\n'.join(output))
    
    def create_categories_index(self):
        """01_Categories.md ìƒì„±"""
        output = []
        output.append("# ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ëª©ë¡\n")
        
        by_category = defaultdict(list)
        for f in self.files_data:
            by_category[f['category']].append(f)
        
        # ì¹´í…Œê³ ë¦¬ ìˆœì„œ ë° ì´ëª¨ì§€
        categories_order = [
            ('AI & Tools', 'ğŸ¤–'),
            ('Language & Interpretation', 'ğŸ—£ï¸'),
            ('Winter Sports', 'ğŸ”ï¸'),
            ('Media & Newsletter', 'ğŸ“°'),
            ('Academic & Research', 'ğŸ“'),
            ('Projects', 'ğŸš€'),
            ('Notes', 'ğŸ“'),
            ('Resources', 'ğŸ”—'),
            ('General Knowledge', 'ğŸ“š')
        ]
        
        for cat, emoji in categories_order:
            files = by_category.get(cat, [])
            if not files:
                continue
            
            output.append(f"## {emoji} {cat} ({len(files)}ê°œ)\n")
            
            # ë‹¨ì–´ ìˆ˜ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            for f in sorted(files, key=lambda x: x['word_count'], reverse=True):
                tags_str = ' '.join(f'#{t}' for t in f['tags'][:3])
                output.append(f"- [[{f['title']}]] - {f['word_count']} words {tags_str}")
            
            output.append("")
        
        self.save_file("01_Categories.md", '\n'.join(output))
    
    def create_tags_index(self):
        """02_Tags_System.md ìƒì„±"""
        output = []
        output.append("# ğŸ·ï¸ íƒœê·¸ ì‹œìŠ¤í…œ\n")
        
        # íƒœê·¸ë³„ ë¬¸ì„œ ìˆ˜ì§‘
        tag_files = defaultdict(list)
        for f in self.files_data:
            for tag in f['tags']:
                tag_files[tag].append(f['title'])
        
        if not tag_files:
            output.append("íƒœê·¸ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
        else:
            # ë¹ˆë„ìˆœ ì •ë ¬
            output.append("## ğŸ“Š íƒœê·¸ ì‚¬ìš© ë¹ˆë„ TOP 20\n")
            for tag, files in sorted(tag_files.items(), key=lambda x: len(x[1]), reverse=True)[:20]:
                output.append(f"### #{tag} ({len(files)}ê°œ)")
                for title in files[:5]:
                    output.append(f"- [[{title}]]")
                if len(files) > 5:
                    output.append(f"- ... ì™¸ {len(files)-5}ê°œ")
                output.append("")
        
        self.save_file("02_Tags_System.md", '\n'.join(output))
    
    def create_action_queue(self):
        """03_Action_Queue.md ìƒì„±"""
        output = []
        output.append("# ğŸ”§ ì •ë¦¬ ì•¡ì…˜ ì œì•ˆ\n")
        output.append(f"> ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        
        # Delete í›„ë³´ (ì§§ê³  ë§í¬ ì—†ìŒ)
        output.append("## ğŸ—‘ï¸ Delete í›„ë³´ (ì§§ê³  ë§í¬ ì—†ìŒ)\n")
        delete_candidates = [f for f in self.files_data 
                            if f['word_count'] < 50 and f['links_in'] == 0 and f['links_out'] == 0]
        if delete_candidates:
            for f in delete_candidates[:10]:
                output.append(f"- [ ] [[{f['title']}]] ({f['word_count']} words)")
        else:
            output.append("ì—†ìŒ\n")
        
        # Archive í›„ë³´ (ì˜¤ë˜ë¨)
        output.append("\n## ğŸ“¦ Archive í›„ë³´ (1ë…„ ì´ìƒ ë¯¸ìˆ˜ì •)\n")
        output.append("- (ë‚ ì§œ í•„í„°ë§ ë¡œì§ ì¶”ê°€ í•„ìš”)\n")
        
        # Split í›„ë³´ (ë„ˆë¬´ ê¹€)
        output.append("## âœ‚ï¸ Split í›„ë³´ (3000+ words)\n")
        split_candidates = [f for f in self.files_data if f['word_count'] > 3000]
        if split_candidates:
            for f in split_candidates[:10]:
                output.append(f"- [ ] [[{f['title']}]] ({f['word_count']} words)")
        else:
            output.append("ì—†ìŒ\n")
        
        self.save_file("03_Action_Queue.md", '\n'.join(output))
    
    def create_statistics(self):
        """04_Statistics.md ìƒì„±"""
        output = []
        output.append("# ğŸ“Š í†µê³„ ë° ë¶„ì„\n")
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        by_category = Counter(f['category'] for f in self.files_data)
        output.append("## ì¹´í…Œê³ ë¦¬ ë¶„í¬\n")
        for cat, count in by_category.most_common():
            pct = count / len(self.files_data) * 100
            output.append(f"- {cat}: {count}ê°œ ({pct:.1f}%)")
        
        # íƒœê·¸ TOP 10
        all_tags = []
        for f in self.files_data:
            all_tags.extend(f['tags'])
        
        if all_tags:
            tag_counts = Counter(all_tags)
            output.append("\n## ìì£¼ ì“°ëŠ” íƒœê·¸ TOP 10\n")
            for tag, count in tag_counts.most_common(10):
                output.append(f"- #{tag}: {count}íšŒ")
        else:
            output.append("\n## ìì£¼ ì“°ëŠ” íƒœê·¸\níƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
        
        # ë¬¸ì„œ í¬ê¸° ë¶„í¬
        output.append("\n## ë¬¸ì„œ í¬ê¸° ë¶„í¬\n")
        word_counts = [f['word_count'] for f in self.files_data]
        if word_counts:
            output.append(f"- í‰ê· : {sum(word_counts)/len(word_counts):.0f} words")
            output.append(f"- ìµœëŒ€: {max(word_counts)} words")
            output.append(f"- ìµœì†Œ: {min(word_counts)} words")
        
        self.save_file("04_Statistics.md", '\n'.join(output))
    
    def save_snapshot(self):
        """ìŠ¤ëƒ…ìƒ· ì €ì¥"""
        history_dir = self.output_path / "history"
        history_dir.mkdir(exist_ok=True)
        
        snapshot_file = history_dir / f"snapshot_{datetime.now().strftime('%Y%m%d')}.json"
        
        with open(snapshot_file, 'w', encoding='utf-8') as f:
            json.dump({
                'date': datetime.now().isoformat(),
                'total_files': len(self.files_data),
                'files': self.files_data
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ìŠ¤ëƒ…ìƒ· ì €ì¥: {snapshot_file.name}")
    
    def save_file(self, filename, content):
        """íŒŒì¼ ì €ì¥"""
        filepath = self.output_path / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ“ {filename} ìƒì„±")


def load_config(config_path="config.txt"):
    """config.txt ë¡œë“œ"""
    config = {}
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    config[key.strip()] = value.strip().strip('"')
        return config
    except Exception as e:
        raise Exception(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")


def main():
    print("\n" + "="*60)
    print("Obsidian Vault Organizer - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¸ë²¤í† ë¦¬")
    print("="*60 + "\n")
    
    # ì„¤ì • ë¡œë“œ
    try:
        config = load_config()
        vault_path = config.get('vault_path')
        output_path = config.get('output_path', 'output')
        
        if not vault_path:
            raise Exception("vault_pathê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        if not os.path.exists(vault_path):
            raise Exception(f"Vault path does not exist: {vault_path}")
        
        print(f"âœ“ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        print(f"  ë³¼íŠ¸ ê²½ë¡œ: {vault_path}")
        print(f"  ì¶œë ¥ ê²½ë¡œ: {output_path}\n")
        
    except Exception as e:
        print(f"âœ— ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        input("\nì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ ì¢…ë£Œ...")
        return
    
    # ë¶„ì„ ì‹œì‘
    start_time = datetime.now()
    
    try:
        analyzer = VaultAnalyzer(vault_path, output_path)
        analyzer.scan_vault()
        analyzer.generate_reports()
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        print("\n" + "="*60)
        print("âœ… ì™„ë£Œ!")
        print("="*60)
        print(f"ì²˜ë¦¬ëœ íŒŒì¼: {len(analyzer.files_data)}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"ì¶œë ¥ ê²½ë¡œ: {output_path}")
        print("="*60 + "\n")
        
        # ìë™ìœ¼ë¡œ enhancer.py ì‹¤í–‰
        print("ğŸš€ Step 2: ê³ ë„í™” ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...\n")
        try:
            import subprocess
            result = subprocess.run(['python3', 'enhancer.py'], 
                                  cwd='/Users/jenkim/Downloads/2026_Antigravity',
                                  capture_output=False)
            if result.returncode == 0:
                print("\nâœ… ëª¨ë“  ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"\nâš ï¸ enhancer.py ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    input("ì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ ì¢…ë£Œ...")


if __name__ == "__main__":
    main()
